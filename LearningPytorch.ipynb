{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LearningPytorch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPL6fzji+ZljEM3j/ocUhXf",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kcelestinomaria/Algo-python/blob/master/LearningPytorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyP6J3Tqmn-B"
      },
      "source": [
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yP6IzdEinLd-"
      },
      "source": [
        "x = torch.Tensor(10).random_(0,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_FqbZG0muaj"
      },
      "source": [
        "x.to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RV2ccFwln_D8"
      },
      "source": [
        "x.to(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4w1kMQxConTb"
      },
      "source": [
        "torch.cuda.is_available()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE-LZXlBoyjK"
      },
      "source": [
        "tensor_1 = torch.tensor([2,6,1,0,9])\r\n",
        "tensor_2 = torch.tensor([[8,10,2,3,7],[2,5,67,3,1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWoN_zEZpV-Y"
      },
      "source": [
        "tensor_1.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4F4_-qLpvT3"
      },
      "source": [
        "tensor_2.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Na5altTzp05Q"
      },
      "source": [
        "tensor = torch.tensor([7,3,0,12,9]).cuda()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-zwC8fZp-DB"
      },
      "source": [
        "When using a GPU-enabled machine, the above modification is implemented to define a tensor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVTVCuQLqRij"
      },
      "source": [
        "example_1 = torch.randn(3,3)\r\n",
        "example_2 = torch.randint(low=0, high=2, size=(3,3)).type(torch.FloatTensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8T83bbUCrjTO"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_oy8uBPmqi4k"
      },
      "source": [
        "Creating dummy data using PyTorch tensors is fairly simple, similar to what you would do in NumPy. For instance, torch.randn() returns a tensor filled with random numbers of the dimensions specified within the parentheses, while torch.randint() returns a tensor filled with integers (the minimum and maximum values can be defined) of the dimensions defined within the parentheses. See the code above\r\n",
        "\r\n",
        "As can be seen, example_1 is a two-dimensional tensor filled with random numbers, with each dimension of size equal to 3, while example_2 is a two-dimensional tensor filled with 0s and 1s (the high parameter is upper-bound exclusive), with each dimension's size equal to 3.\r\n",
        "\r\n",
        "Any tensor filled with integers must be converted into floats so that we can feed it to any PyTorch model(That's why we used torch.FloatTensor)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OTLr5mJZqoXA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yvralm9bsswW"
      },
      "source": [
        "The PyTorch autograd Library\r\n",
        "The autograd library consists of a technique called automatic differentiation. Its purpose is to numerically calculate the derivative of a function. This is crucial for a concept we will learn about in the next chapter called backward propagation, which is carried out while training a neural network.\r\n",
        "\r\n",
        "The derivative (also known as the gradient) of an element refers to the rate of change of that element in a given time step. In deep learning, gradients refer to the dimension and magnitude in which the parameters of the neural network must be updated in a training step in order to minimize the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jn5nCUOns3dS"
      },
      "source": [
        "a = torch.tensor([5.0, 8.9], requires_grad=True)\r\n",
        "b = torch.tensor([9.2, 1.0])\r\n",
        "\r\n",
        "ab = ((a + b)**2).sum()\r\n",
        "ab.backward()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VGSqYOmOuCnb"
      },
      "source": [
        "In the preceding code, two tensors were created. We use the requires_grad argument here to tell PyTorch to calculate the gradients of that tensor. However, when building your neural network, this argument is not required.\r\n",
        "\r\n",
        "Next, a function was defined using the values of both tensors. Finally, the backward() function was used to calculate the gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUCbgdMjuOy0"
      },
      "source": [
        "By printing the gradients for both a and b, it is possible to confirm that they were only calculated for the first variable (a), while for the second one (b), it throws an error:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gr8N1-O5uQUK"
      },
      "source": [
        "print(a.grad.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZSAlk6quWeS"
      },
      "source": [
        "print(b.grad.data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spTRYkZWumuS"
      },
      "source": [
        "The autograd library alone can be used to build simple neural networks, considering that the trickier part (the calculation of gradients) has been taken care of. However, this methodology can be troublesome, hence the introduction of the nn module.\r\n",
        "\r\n",
        "The nn module is a complete PyTorch module used to create and train neural networks, which, through the use of different elements, allows for simple and complex developments. For instance, the Sequential() container allows for the easy creation of network architectures that follow a sequence of predefined modules (or layers) without the need for much knowledge of defining network architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f8A5H33u2_t"
      },
      "source": [
        "This module also has the capability to define the loss function to evaluate the model and many more advanced features that will be discussed in this course.\r\n",
        "\r\n",
        "The process of building a neural network architecture as a sequence of predefined modules can be achieved in just a couple of lines, as shown below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oyk7OXSKu6tz"
      },
      "source": [
        "import torch.nn as nn\r\n",
        "\r\n",
        "model = nn.Sequential(nn.Linear(input_units, hidden_units),\r\n",
        "                      nn.ReLU(),\r\n",
        "                      nn.Linear(hidden_units, output_units),\r\n",
        "                      nn.Sigmoid())\r\n",
        "loss_funct = nn.MSELoss()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_iY3N43wab0"
      },
      "source": [
        "The process of building a neural network architecture as a sequence of predefined modules can be achieved in just a couple of lines, as shown here:\r\n",
        "\r\n",
        "import torch.nn as nn\r\n",
        "model = nn.Sequential(nn.Linear(input_units, hidden_units),\r\n",
        "                      nn.ReLU(),\r\n",
        "                      nn.Linear(hidden_units, output_units),\r\n",
        "                      nn.Sigmoid())\r\n",
        "loss_funct = nn.MSELoss()\r\n",
        "First, the module is imported. And then, the model architecture is defined. input_units refers to the number of features that the input data contains, hidden_units refers to the number of nodes of the hidden layer, and output_units refers to the number of nodes of the output layer.\r\n",
        "\r\n",
        "As can be seen in the preceding code, the architecture of the network contains one hidden layer, followed by a ReLU activation function and an output layer, followed by a sigmoid activation function, making it a two-layer network.\r\n",
        "\r\n",
        "Finally, the loss function is defined as the Mean Squared Error (MSE).\r\n",
        "\r\n",
        "Note: The most popular loss functions for different data problems will be explained throughout this course. To create models that do not follow a sequence of existing modules, custom nn modules are used. We'll introduce these later in this course."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADZMgMRJwbcs"
      },
      "source": [
        "input_units = 10\r\n",
        "output_units = 1\r\n",
        "\r\n",
        "model = nn.Sequential(nn.Linear(input_units, output_units),\r\n",
        "                      nn.Sigmoid())\r\n",
        "\r\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOYp-qfOz7rm"
      },
      "source": [
        "#Define the loss function as the MSE and store it in a variable named loss_funct\r\n",
        "loss_funct = nn.MSELoss()\r\n",
        "print(loss_funct)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vpg3RD4a0DBV"
      },
      "source": [
        "Above, is a successfully created single-network architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v4h_BP_00cnE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoTJ1KDlAemQ"
      },
      "source": [
        "**The PyTorch optim Package**\r\n",
        "\r\n",
        "The optim package is used to define the optimizer that will be used to update the parameters in each iteration (which will be further explained in the following chapters) using the gradients calculated by the autograd module. Here, it is possible to choose from different optimization algorithms that are available, such as Adam, Stochastic Gradient Descent (SGD), and Root Mean Square Propagation (RMSprop), among others"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOi1zCb8Ag4p"
      },
      "source": [
        "import torch\r\n",
        "\r\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCJGFUdwBRrJ"
      },
      "source": [
        "Here, the model.parameters() argument refers to the weights and biases from the model that were previously created, while lr refers to the learning rate, which was set to 0.01.\r\n",
        "\r\n",
        "Weights are the values that are used to determine the level of importance of a bit of information in a general context. This means that every bit of information has an accompanying weight for every neuron in the network. Moreover, bias is similar to the intercept element that’s added to a linear function and is used to adjust the output from the computation of relevance in a given neuron.\r\n",
        "\r\n",
        "The learning rate is a running parameter that’s used in optimization processes to determine the extent of the steps to be taken toward minimizing the loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HKF256iWB8AZ"
      },
      "source": [
        "Next, the process of running the optimization for 100 iterations is shown below, which, as you can see, uses the model created by the nn module and the gradients calculated by the autograd library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tXm3GpDfA6IJ"
      },
      "source": [
        "for i in range(100):\r\n",
        "  #We make a call to the model to perform a prediction\r\n",
        "  y_pred = model(x)\r\n",
        "\r\n",
        "  #Calulation of loss function based on y_pred and y\r\n",
        "  loss = loss_funct(y_pred, y)\r\n",
        "\r\n",
        "  #Zero the gradients so that the previous ones do not accumulate\r\n",
        "  optimizer.zero_grad()\r\n",
        "\r\n",
        "  #Calculate the gradients of the loss function\r\n",
        "  loss.backward()\r\n",
        "\r\n",
        "  #Call to the optimizer to perform an update of the parameters\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8xmLBWRD4cb"
      },
      "source": [
        "For each iteration, the model is called to obtain a prediction (y_pred). This prediction and the ground truth values (y) are fed to the loss functions in order to determine the ability of the model to approximate to the ground truth.\r\n",
        "\r\n",
        "Next, the gradients are zeroed, and the gradients of the loss function are calculated using the backward() function.\r\n",
        "\r\n",
        "Finally, the step() function is called to update the weights and biases based on the optimization algorithm and the gradients calculated previously."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22GJW62QHnSu"
      },
      "source": [
        "Next, we will learn how to train the single-layer network from the previous exercise, using PyTorch's optim package. Considering that we will use dummy data as input, training the network won't solve a data problem, but it will be performed for learning purposes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zk0WQEU3Hsoo"
      },
      "source": [
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "#Let's create dummy data, x being the input and y being the output\r\n",
        "x = torch.randn(20, 10)\r\n",
        "y = torch.randint(0, 2, (20, 1)).type(torch.FloatTensor)\r\n",
        "\r\n",
        "input_units = 10\r\n",
        "output_units = 1\r\n",
        "\r\n",
        "model = nn.Sequential(nn.Linear(input_units, output_units),\r\n",
        "                      nn.Sigmoid())\r\n",
        "\r\n",
        "\r\n",
        "#We are going to use the Adam optimizer\r\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\r\n",
        "\r\n",
        "\r\n",
        "losses = []\r\n",
        "for i in range(20):\r\n",
        "  y_pred = model(x)\r\n",
        "\r\n",
        "  loss = loss_funct(y_pred, y)\r\n",
        "\r\n",
        "  losses.append(loss.item())\r\n",
        "\r\n",
        "  optimizer.zero_grad()\r\n",
        "\r\n",
        "  loss.backward()\r\n",
        "\r\n",
        "  optimizer.step()\r\n",
        "\r\n",
        "  if i%5 == 0:\r\n",
        "    print(i, loss.item())\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6V8w-XQD7_e"
      },
      "source": [
        "%matplotlib inline\r\n",
        "plt.plot(range(0, 20), losses)\r\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}